[{"authors":["admin"],"categories":null,"content":"\u0026hellip;an Applied Mathematics PhD Candidate in the Data Science group of the Mathematical Institute at the University of Oxford advised by Prof. Jared Tanner.\nMy PhD research focuses on theories of Deep Learing. In particular, I am interested in the study of infinitely wide neural networks and possible optimal initialisation schemes.\nI also have some interests in Geometrical Machine Learning methods, as such as applications of Deep Learning models to solve public health issues.\nEmail : thiziri [dot] naitsaada [at] maths [dot] ox [dot] ac [dot] uk\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://thizirinait.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"\u0026hellip; an Applied Mathematics PhD Candidate in the Data Science group of the Mathematical Institute at the University of Oxford advised by Prof. Jared Tanner.\nMy PhD research focuses on theories of Deep Learing. In particular, I am interested in the study of infinitely wide neural networks and possible optimal initialisation schemes.\nI also have some interests in Geometrical Machine Learning methods, as such as applications of Deep Learning models to solve public health issues.(e.","tags":null,"title":"Thiziri Nait Saada","type":"authors"},
 {"authors":["T. Nait Saada and J. Tanner"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"f08bd94507a75043830a2ef87cbba928","permalink":"https://thizirinait.github.io/publication/EOC-lowrank-2023/","publishdate":"2023-01-26T10:52:54+01:00","relpermalink":"/publication/EOC-lowrank-2023/","section":"publication","summary":"The edge-of-chaos dynamics of wide randomly initialized low-rank feedforward networks are an- alyzed. Formulae for the optimal weight and bias variances are extended from the full-rank to low- rank setting and are shown to follow from mul- tiplicative scaling. The principle second order effect, the variance of the input-output Jacobian, is derived and shown to increase as the rank to width ratio decreases. These results inform prac- titioners how to randomly initialize feedforward networks with a reduced number of learnable pa- rameters while in the same ambient dimension, allowing reductions in the computational cost and memory constraints of the associated network.","tags":[],"title":"On the Initialisation of Wide Low-Rank Feedforward Neural Networks.","type":"publication"}}]
